import streamlit as st
import tensorflow as tf
import cv2
import numpy as np
from PIL import Image
from streamlit_drawable_canvas import st_canvas

# 1. é é¢è¨­å®š (ä¿æŒåŸæœ¬è¨­å®š)
st.set_page_config(page_title="AIæ‰‹å¯«è¾¨è­˜APP", layout="wide")

# åˆå§‹åŒ–çµ±è¨ˆæ•¸æ“š (ä½¿ç”¨ Session State)
if 'total_count' not in st.session_state:
    st.session_state.total_count = 0
if 'correct_count' not in st.session_state:
    st.session_state.correct_count = 0

# --- CSS è£œå¼· (ä¿æŒåŸæœ¬ä¿®å¾©è¡Œå‹•ç«¯å•é¡Œçš„é‚è¼¯) ---
st.markdown(
    """
    <style>
    html, body, [data-testid="stAppViewContainer"] {
        overscroll-behavior-y: contain !important;
        overflow: hidden !important;
    }
    canvas { touch-action: none !important; }
    </style>
    """,
    unsafe_allow_html=True
)

# æ¨™é¡Œç¶­æŒä¸è®Š
st.title("ğŸ”¢ AIæ‰‹å¯«è¾¨è­˜APP") 
st.markdown("""
##### ğŸ’¡ **ä½¿ç”¨èªªæ˜ï¼š**
1. **å¤šå…ƒè¼¸å…¥æ¨¡å¼**ï¼šæ”¯æ´ç•«æ¿æ‰‹å¯«ã€æ‹ç…§ã€åœ–ç‰‡æª”æ¡ˆä¸Šå‚³ã€‚
2. **æœ€ä½³è¾¨è­˜å»ºè­°**ï¼šå»ºè­°ä½¿ç”¨è¼ƒç²—çš„ç­†æ›¸å¯«ï¼ˆå¦‚é¦¬å…‹ç­†ï¼‰ï¼Œä¸¦åœ¨å…‰æºå……è¶³çš„æƒ…æ³ä¸‹æ‹æ”ã€‚
3. **æ€§èƒ½å„ªåŒ–æ‰‹è…•**ï¼šè‹¥è¾¨è­˜ä¸ä½³ï¼Œå¯ä½¿ç”¨å´é¢æ¿å¾®èª¿åƒæ•¸æé«˜è¾¨è­˜æˆåŠŸç‡ã€‚
4. **æ‰‹å¯«æ³¨æ„äº‹é …**ï¼šæ‰‹å¯«éƒ¨åˆ†å‹¿å¤ªé è¿‘é‚Šæ¡†ï¼Œä»¥å…è¾¨è­˜éŒ¯èª¤ã€‚
""")
st.divider()

# 2. è¼‰å…¥æ¨¡å‹
@st.cache_resource
def load_my_model():
    return tf.keras.models.load_model('mnist_model.h5')

try:
    model = load_my_model()
    st.sidebar.success("âœ… AI æ¨¡å‹å·²å°±ç·’")
except Exception as e:
    st.sidebar.error("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")

# 3. å´é‚Šæ¬„
st.sidebar.header("ğŸ› ï¸ ç³»çµ±åŠŸèƒ½è¨­å®š")
option = st.sidebar.radio("ğŸ“¸ é¸æ“‡è¼¸å…¥ä¾†æºï¼š", ("æ‰‹å¯«ç•«æ¿æ¨¡å¼", "ä½¿ç”¨ç›¸æ©Ÿæ‹ç…§", "ä¸Šå‚³åœ–ç‰‡æª”"))

# æ­·å²çµ±è¨ˆå€ (ä½æ–¼å´é‚Šæ¬„)
st.sidebar.divider()
st.sidebar.subheader("ğŸ“Š æ­·å²è¾¨è­˜çµ±è¨ˆ")
if st.session_state.total_count > 0:
    acc = (st.session_state.correct_count / st.session_state.total_count) * 100
    st.sidebar.write(f"ç¸½è¾¨è­˜æ¬¡æ•¸: {st.session_state.total_count}")
    st.sidebar.write(f"æ­£ç¢ºæ¬¡æ•¸: {st.session_state.correct_count}")
    st.sidebar.metric("æ­·å²æ­£ç¢ºç‡", f"{acc:.2f}%")
    if st.sidebar.button("ğŸ—‘ï¸ åˆªé™¤çµ±è¨ˆç´€éŒ„"):
        st.session_state.total_count = 0
        st.session_state.correct_count = 0
        st.rerun()
else:
    st.sidebar.write("å°šç„¡çµ±è¨ˆè³‡æ–™")

st.sidebar.divider()
st.sidebar.write("ğŸ” è¾¨è­˜åƒæ•¸å¾®èª¿ (æ‹ç…§/ä¸Šå‚³å°ˆç”¨)")
min_area = st.sidebar.slider("1. é›œè¨Šéæ¿¾å¼·åº¦", 100, 1500, 300)
sensitivity = st.sidebar.slider("2. æ•æ‰éˆæ•åº¦", 1, 25, 12)
thickness = st.sidebar.slider("3. å­—é«”åŠ ç²—ç¨‹åº¦", 1, 5, 2)

# 4. å½±åƒè™•ç†å‡½æ•¸ (åŠ å…¥ä¿¡å¿ƒåº¦è¨ˆç®—)
def process_and_predict(img_gray, is_canvas=False):
    if is_canvas:
        _, thresh = cv2.threshold(img_gray, 1, 255, cv2.THRESH_BINARY)
    else:
        enhanced = cv2.convertScaleAbs(img_gray, alpha=1.5, beta=0)
        blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY_INV, 11, sensitivity)
        kernel = np.ones((3,3), np.uint8)
        thresh = cv2.dilate(thresh, kernel, iterations=thickness)

    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    valid_contours = sorted([c for c in contours if cv2.contourArea(c) > min_area], 
                            key=lambda c: cv2.boundingRect(c)[0])
    
    if not valid_contours:
        return None, None, None

    results = []
    confidences = []
    roi_images = []
    for cnt in valid_contours:
        x, y, w, h = cv2.boundingRect(cnt)
        pad = 20
        sq_size = max(w, h) + pad * 2
        roi = thresh[y:y+h, x:x+w]
        digit_canvas = np.zeros((sq_size, sq_size), dtype="uint8")
        digit_canvas[pad:pad+h, pad:pad+w] = roi
        final_img = cv2.resize(digit_canvas, (28, 28), interpolation=cv2.INTER_AREA)
        
        input_data = final_img.astype('float32') / 255.0
        input_data = np.expand_dims(input_data, axis=(0, -1))
        prediction = model.predict(input_data, verbose=0)
        
        results.append(np.argmax(prediction))
        confidences.append(np.max(prediction))
        roi_images.append(final_img)
        
    return results, confidences, roi_images

# 5. æ¨¡å¼åˆ‡æ›é‚è¼¯
if option == "æ‰‹å¯«ç•«æ¿æ¨¡å¼":
    st.write("### âœï¸ è«‹åœ¨é»‘è‰²ç•«æ¿å…§å¯«å…¥æ•¸å­—ï¼š")
    
    # ç•«æ¿å·¥å…·é¸æ“‡
    tool_col, _ = st.columns([2, 2])
    with tool_col:
        drawing_mode = st.radio("ğŸ–Œï¸ å·¥å…·é¸æ“‡ï¼š", ("ç•«ç­†æ¨¡å¼", "æ©¡çš®æ“¦æ¨¡å¼"), horizontal=True)
    
    # æ ¹æ“šé¸æ“‡è¨­å®šç•«æ¿æ¨¡å¼èˆ‡ç²—ç´°
    real_mode = "freedraw" if drawing_mode == "ç•«ç­†æ¨¡å¼" else "eraser"
    stroke_w = 15 if drawing_mode == "ç•«ç­†æ¨¡å¼" else 30

    canvas_result = st_canvas(
        fill_color="rgba(255, 255, 255, 0.3)",
        stroke_width=stroke_w,
        stroke_color="#FFFFFF",
        background_color="#000000",
        width=700,
        height=500, # ç¶­æŒåŠ å¤§å¾Œçš„å°ºå¯¸
        drawing_mode=real_mode,
        key="canvas",
    )
    
    if canvas_result.image_data is not None:
        img_raw = canvas_result.image_data.astype('uint8')
        img_gray = cv2.cvtColor(img_raw, cv2.COLOR_RGBA2GRAY)
        
        if st.button("ğŸš€ é€²è¡Œ AI è¾¨è­˜"):
            res, confs, imgs = process_and_predict(img_gray, is_canvas=True)
            if res:
                final_res_str = ''.join(map(str, res))
                st.success(f"## æœ€çµ‚è¾¨è­˜çµæœï¼š {final_res_str}")
                
                # é¡¯ç¤ºåˆ‡åˆ†å¾Œçš„æ•¸å­—èˆ‡ä¿¡å¿ƒåº¦
                cols = st.columns(len(imgs))
                for i, im in enumerate(imgs):
                    with cols[i]:
                        st.image(im, caption=f"é æ¸¬: {res[i]}")
                        st.write(f"ä¿¡å¿ƒåº¦: {confs[i]*100:.1f}%")
                
                # å ±éŒ¯å›é¥‹èˆ‡çµ±è¨ˆå€
                st.divider()
                st.subheader("ğŸš© è¾¨è­˜å›é¥‹èˆ‡çµ±è¨ˆ")
                with st.form("feedback_form"):
                    st.write("å¦‚æœè¾¨è­˜éŒ¯èª¤ï¼Œè«‹è¼¸å…¥æ­£ç¢ºæ•¸å€¼ï¼š")
                    user_correct = st.text_input("æ­£ç¢ºçš„æ•¸å€¼æ‡‰è©²æ˜¯ï¼š", value=final_res_str)
                    submit = st.form_submit_button("æäº¤ä¸¦ç´€éŒ„çµ±è¨ˆ")
                    
                    if submit:
                        st.session_state.total_count += 1
                        if user_correct == final_res_str:
                            st.session_state.correct_count += 1
                            st.success("âœ… å·²ç´€éŒ„ï¼æ­£ç¢ºç‡æå‡ä¸­ï¼")
                        else:
                            st.warning("âš ï¸ å·²ç´€éŒ„éŒ¯èª¤å›é¥‹ï¼Œå°‡ä½œç‚ºå„ªåŒ–åƒè€ƒã€‚")
                        st.rerun()
            else:
                st.warning("è«‹åœ¨ç•«æ¿ä¸Šæ›¸å¯«æ•¸å­—ã€‚")

elif option == "ä½¿ç”¨ç›¸æ©Ÿæ‹ç…§" or option == "ä¸Šå‚³åœ–ç‰‡æª”":
    img_file = st.camera_input("ğŸ“¸ ç«‹å³æ‹æ”æ•¸å­—") if option == "ä½¿ç”¨ç›¸æ©Ÿæ‹ç…§" else st.file_uploader("ğŸ“ ä¸Šå‚³åœ–ç‰‡æª”æ¡ˆ", type=["jpg", "png", "jpeg"])
    
    if img_file:
        image = Image.open(img_file)
        img_array = np.array(image.convert('RGB'))
        img_gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        st.write("### ğŸ–¼ï¸ è™•ç†ç´°ç¯€ï¼š")
        st.image(image, width=400)
        
        res, confs, imgs = process_and_predict(img_gray)
        if res:
            st.divider()
            st.success(f"## ğŸ”¢ æœ€çµ‚è¾¨è­˜çµæœï¼š {''.join(map(str, res))}")
            cols = st.columns(min(len(imgs), 10))
            for i, im in enumerate(imgs):
                with cols[i]:
                    st.image(im, caption=f"é æ¸¬: {res[i]} ({confs[i]*100:.1f}%)")
        else:
            st.warning("åµæ¸¬ä¸åˆ°æ•¸å­—ï¼Œè«‹è©¦è‘—èª¿æ•´å´é¢æ¿åƒæ•¸ã€‚")