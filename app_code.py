import streamlit as st
import tensorflow as tf
import cv2
import numpy as np
from PIL import Image
from streamlit_drawable_canvas import st_canvas

# 1. é é¢è¨­å®š
st.set_page_config(page_title="AIæ‰‹å¯«è¾¨è­˜APP", layout="wide")

# åˆå§‹åŒ–çµ±è¨ˆèˆ‡ç´€éŒ„ (åƒ…å­˜åœ¨æ–¼ç•¶å‰é é¢ Session)
if 'total_count' not in st.session_state:
    st.session_state.total_count = 0
if 'correct_count' not in st.session_state:
    st.session_state.correct_count = 0
if 'feedback_history' not in st.session_state:
    st.session_state.feedback_history = [] # ç”¨ä¾†å­˜ä»‹é¢é¡¯ç¤ºçš„ç´€éŒ„

# --- CSS è£œå¼· ---
st.markdown(
    """
    <style>
    html, body, [data-testid="stAppViewContainer"] {
        overscroll-behavior-y: contain !important;
        overflow: hidden !important;
    }
    canvas { touch-action: none !important; }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ”¢ AIæ‰‹å¯«è¾¨è­˜APP")
st.markdown("""
##### ğŸ’¡ **ä½¿ç”¨èªªæ˜ï¼š**
1. **å¤šå…ƒè¼¸å…¥æ¨¡å¼**ï¼šæ”¯æ´ç•«æ¿æ‰‹å¯«ã€æ‹ç…§ã€åœ–ç‰‡æª”æ¡ˆä¸Šå‚³ã€‚
2. **æœ€ä½³è¾¨è­˜å»ºè­°**ï¼šå»ºè­°ä½¿ç”¨è¼ƒç²—çš„ç­†æ›¸å¯«ï¼Œä¸¦åœ¨å…‰æºå……è¶³çš„æƒ…æ³ä¸‹æ‹æ”ã€‚
3. **æ€§èƒ½å„ªåŒ–æ‰‹è…•**ï¼šè‹¥è¾¨è­˜ä¸ä½³ï¼Œå¯ä½¿ç”¨å´é¢æ¿å¾®èª¿åƒæ•¸ã€‚
4. **æ‰‹å¯«æ³¨æ„äº‹é …**ï¼šæ‰‹å¯«éƒ¨åˆ†å‹¿å¤ªé è¿‘é‚Šæ¡†ï¼Œä»¥å…è¾¨è­˜éŒ¯èª¤ã€‚
""")
st.divider()

# 2. è¼‰å…¥æ¨¡å‹
@st.cache_resource
def load_my_model():
    return tf.keras.models.load_model('mnist_model.h5')

try:
    model = load_my_model()
    st.sidebar.success("âœ… AI æ¨¡å‹å·²å°±ç·’")
except Exception as e:
    st.sidebar.error("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")

# 3. å´é‚Šæ¬„èˆ‡çµ±è¨ˆé¡¯ç¤º
st.sidebar.header("ğŸ› ï¸ ç³»çµ±åŠŸèƒ½è¨­å®š")
option = st.sidebar.radio("ğŸ“¸ é¸æ“‡è¼¸å…¥ä¾†æºï¼š", ("æ‰‹å¯«ç•«æ¿æ¨¡å¼", "ä½¿ç”¨ç›¸æ©Ÿæ‹ç…§", "ä¸Šå‚³åœ–ç‰‡æª”"))

st.sidebar.divider()
st.sidebar.subheader("ğŸ“Š æœ¬æ¬¡åŸ·è¡Œçµ±è¨ˆ")
if st.session_state.total_count > 0:
    acc = (st.session_state.correct_count / st.session_state.total_count) * 100
    st.sidebar.write(f"ç¸½è¾¨è­˜æ¬¡æ•¸: {st.session_state.total_count}")
    st.sidebar.metric("ç›®å‰æ­£ç¢ºç‡", f"{acc:.2f}%")
    
    # --- æ–°å¢ï¼šåœ¨ä»‹é¢é¡¯ç¤ºåé¥‹ç´€éŒ„æ¸…å–® ---
    with st.sidebar.expander("ğŸ“ æŸ¥çœ‹åé¥‹ç´€éŒ„è©³æƒ…", expanded=True):
        for i, entry in enumerate(reversed(st.session_state.feedback_history)):
            color = "green" if entry['is_correct'] else "red"
            st.markdown(f"{i+1}. AI:[{entry['pred']}] â†’ å¯¦éš›:[{entry['actual']}] :{color}[{'â—' if entry['is_correct'] else 'X'}]")

    if st.sidebar.button("ğŸ—‘ï¸ åˆªé™¤çµ±è¨ˆç´€éŒ„"):
        st.session_state.total_count = 0
        st.session_state.correct_count = 0
        st.session_state.feedback_history = []
        st.rerun()
else:
    st.sidebar.write("å°šç„¡çµ±è¨ˆè³‡æ–™")

st.sidebar.divider()
st.sidebar.write("ğŸ” è¾¨è­˜åƒæ•¸å¾®èª¿")
min_area = st.sidebar.slider("1. é›œè¨Šéæ¿¾å¼·åº¦", 100, 1500, 300)
sensitivity = st.sidebar.slider("2. æ•æ‰éˆæ•åº¦", 1, 25, 12)
thickness = st.sidebar.slider("3. å­—é«”åŠ ç²—ç¨‹åº¦", 1, 5, 2)

# 4. å½±åƒè™•ç†å‡½æ•¸
def process_and_predict(img_gray, is_canvas=False):
    if is_canvas:
        _, thresh = cv2.threshold(img_gray, 1, 255, cv2.THRESH_BINARY)
    else:
        enhanced = cv2.convertScaleAbs(img_gray, alpha=1.5, beta=0)
        blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY_INV, 11, sensitivity)
        kernel = np.ones((3,3), np.uint8)
        thresh = cv2.dilate(thresh, kernel, iterations=thickness)

    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    valid_contours = sorted([c for c in contours if cv2.contourArea(c) > min_area], 
                            key=lambda c: cv2.boundingRect(c)[0])
    
    if not valid_contours: return None, None, None

    results, confidences, roi_images = [], [], []
    for cnt in valid_contours:
        x, y, w, h = cv2.boundingRect(cnt)
        roi = thresh[y:y+h, x:x+w]
        pad = 30 # è§£æ±º 1 çœ‹æˆ 6 çš„ Padding å„ªåŒ–
        digit_canvas = cv2.copyMakeBorder(roi, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=0)
        final_img = cv2.resize(digit_canvas, (28, 28), interpolation=cv2.INTER_AREA)
        input_data = final_img.astype('float32') / 255.0
        input_data = np.expand_dims(input_data, axis=(0, -1))
        prediction = model.predict(input_data, verbose=0)
        results.append(np.argmax(prediction))
        confidences.append(np.max(prediction))
        roi_images.append(final_img)
    return results, confidences, roi_images

# 5. æ¨¡å¼åˆ‡æ›é‚è¼¯
if option == "æ‰‹å¯«ç•«æ¿æ¨¡å¼":
    st.write("### âœï¸ è«‹åœ¨é»‘è‰²ç•«æ¿å…§å¯«å…¥æ•¸å­—ï¼š")
    canvas_result = st_canvas(
        stroke_width=15, stroke_color="#FFFFFF", background_color="#000000",
        width=700, height=500, drawing_mode="freedraw", key="canvas_stable",
    )
    
    if canvas_result.image_data is not None:
        if st.button("ğŸš€ é€²è¡Œ AI è¾¨è­˜"):
            img_raw = canvas_result.image_data.astype('uint8')
            img_gray = cv2.cvtColor(img_raw, cv2.COLOR_RGBA2GRAY)
            res, confs, imgs = process_and_predict(img_gray, is_canvas=True)
            if res:
                final_str = ''.join(map(str, res))
                st.session_state['current_pred'] = final_str
                st.success(f"## æœ€çµ‚è¾¨è­˜çµæœï¼š {final_str}")
                cols = st.columns(len(imgs))
                for i, im in enumerate(imgs):
                    with cols[i]:
                        st.image(im, caption=f"é æ¸¬: {res[i]} ({confs[i]*100:.1f}%)")

    # åé¥‹å€
    if 'current_pred' in st.session_state:
        st.divider()
        st.subheader("ğŸš© è¾¨è­˜å›é¥‹")
        c1, c2 = st.columns([3, 1])
        with c1:
            correct_ans = st.text_input("å¦‚æœæœ‰èª¤ï¼Œè«‹è¼¸å…¥æ­£ç¢ºç­”æ¡ˆï¼š", value=st.session_state['current_pred'])
        with c2:
            st.write(" ") # å°é½Š
            if st.button("æäº¤å›é¥‹"):
                is_correct = (st.session_state['current_pred'] == correct_ans)
                st.session_state.total_count += 1
                if is_correct: st.session_state.correct_count += 1
                
                # å­˜å…¥ç´€éŒ„æ¸…å–®ä»¥ä¾›ä»‹é¢é¡¯ç¤º
                st.session_state.feedback_history.append({
                    "pred": st.session_state['current_pred'],
                    "actual": correct_ans,
                    "is_correct": is_correct
                })
                
                del st.session_state['current_pred']
                st.rerun()

elif option == "ä½¿ç”¨ç›¸æ©Ÿæ‹ç…§" or option == "ä¸Šå‚³åœ–ç‰‡æª”":
    img_file = st.camera_input("ğŸ“¸ ç«‹å³æ‹æ”æ•¸å­—") if option == "ä½¿ç”¨ç›¸æ©Ÿæ‹ç…§" else st.file_uploader("ğŸ“ ä¸Šå‚³åœ–ç‰‡æª”æ¡ˆ", type=["jpg", "png", "jpeg"])
    if img_file:
        image = Image.open(img_file)
        img_array = np.array(image.convert('RGB'))
        img_gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        res, confs, imgs = process_and_predict(img_gray)
        if res:
            final_str = ''.join(map(str, res))
            st.session_state['current_pred'] = final_str
            st.success(f"## ğŸ”¢ æœ€çµ‚è¾¨è­˜çµæœï¼š {final_str}")
            cols = st.columns(min(len(imgs), 10))
            for i, im in enumerate(imgs):
                with cols[i]:
                    st.image(im, caption=f"é æ¸¬: {res[i]} ({confs[i]*100:.1f}%)")